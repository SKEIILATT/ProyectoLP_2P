"""
Scraper para Google Scholar - Papers sobre deserciÃ³n estudiantil
"""
import time
import json
from scholarly import scholarly, ProxyGenerator
from typing import List, Dict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ScholarScraper:
    def __init__(self, use_proxy=False):
        """Inicializa el scraper con opciÃ³n de proxy para evitar bloqueos"""
        if use_proxy:
            pg = ProxyGenerator()
            pg.FreeProxies()
            scholarly.use_proxy(pg)
    
    def search_papers(self, query: str, max_results: int = 10) -> List[Dict]:
        """
        Busca papers en Google Scholar
        
        Args:
            query: TÃ©rmino de bÃºsqueda
            max_results: NÃºmero mÃ¡ximo de resultados
            
        Returns:
            Lista de diccionarios con metadata de papers
        """
        papers = []
        try:
            search_query = scholarly.search_pubs(query)
            
            for i, result in enumerate(search_query):
                if i >= max_results:
                    break
                
                try:
                    # Extraer informaciÃ³n del paper
                    paper_data = {
                        'title': result.get('bib', {}).get('title', 'N/A'),
                        'abstract': result.get('bib', {}).get('abstract', 'N/A'),
                        'year': result.get('bib', {}).get('pub_year', 'N/A'),
                        'authors': result.get('bib', {}).get('author', []),
                        'citations': result.get('num_citations', 0),
                        'url': result.get('pub_url', 'N/A'),
                        'venue': result.get('bib', {}).get('venue', 'N/A'),
                        'query': query
                    }
                    
                    papers.append(paper_data)
                    logger.info(f"âœ“ ExtraÃ­do: {paper_data['title'][:50]}...")
                    
                    # Pausa para evitar bloqueos
                    time.sleep(2)
                    
                except Exception as e:
                    logger.warning(f"Error al procesar paper {i}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error en bÃºsqueda '{query}': {e}")
        
        return papers
    
    def scrape_multiple_queries(self, queries: List[str], papers_per_query: int = 7) -> List[Dict]:
        """
        Realiza mÃºltiples bÃºsquedas y combina resultados
        
        Args:
            queries: Lista de tÃ©rminos de bÃºsqueda
            papers_per_query: Papers a extraer por consulta
            
        Returns:
            Lista consolidada de papers
        """
        all_papers = []
        
        for query in queries:
            logger.info(f"\nğŸ” Buscando: '{query}'")
            papers = self.search_papers(query, papers_per_query)
            all_papers.extend(papers)
            
            # Pausa entre consultas
            time.sleep(5)
        
        # Eliminar duplicados por tÃ­tulo
        unique_papers = []
        seen_titles = set()
        
        for paper in all_papers:
            title = paper['title'].lower()
            if title not in seen_titles and title != 'n/a':
                seen_titles.add(title)
                unique_papers.append(paper)
        
        logger.info(f"\nâœ… Total de papers Ãºnicos: {len(unique_papers)}")
        return unique_papers
    
    def save_to_json(self, papers: List[Dict], filepath: str):
        """Guarda papers en formato JSON"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(papers, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ Guardado en: {filepath}")


# Script de ejecuciÃ³n
if __name__ == "__main__":
    scraper = ScholarScraper(use_proxy=False)
    
    # Consultas especÃ­ficas sobre deserciÃ³n estudiantil
    queries = [
        "student dropout prediction machine learning",
        "deserciÃ³n estudiantil predicciÃ³n",
        "educational data mining retention",
        "student attrition factors higher education",
        "early warning systems student dropout"
    ]
    
    # Extraer papers
    papers = scraper.scrape_multiple_queries(queries, papers_per_query=5)
    
    # Guardar resultados
    import os
    os.makedirs('datos/papers_academicos', exist_ok=True)
    scraper.save_to_json(papers, 'datos/papers_academicos/papers_desercion.json')
    
    print(f"\nğŸ‰ Scraping completado: {len(papers)} papers extraÃ­dos")